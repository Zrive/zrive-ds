{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace preparation\n",
    "\n",
    "## Installing and Importing Add-ins and libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add scikit-learn\n",
    "!poetry add seaborn\n",
    "!poetry add numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve,roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing now our files from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-23 14:06:02  761678715 groceries/box_builder_dataset/feature_frame.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://zrive-ds-data/groceries/box_builder_dataset/ --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then copying in local the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://zrive-ds-data/groceries/box_builder_dataset/feature_frame.csv to ../../../../../../mnt/c/Users/Daniel Sánchez/Desktop/ZRIVE DS/src/module_3/feature_frame.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://zrive-ds-data/groceries/box_builder_dataset/feature_frame.csv \"/mnt/c/Users/Daniel Sánchez/Desktop/ZRIVE DS/src/module_3/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath=\"/mnt/c/Users/Daniel Sánchez/Desktop/ZRIVE DS/src/module_3/feature_frame.csv\"\n",
    "df=pd.read_csv(dfpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cols=[\"variant_id\",\"order_id\",\"user_id\",\"created_at\",\"order_date\"]\n",
    "label_col=[\"outcome\"]\n",
    "feature_cols=[col for col in df.columns if col not in info_cols +[label_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols=[\"product_type\",\"vendor\"]\n",
    "binary_cols=[\"ordered_before\",\"abandoned_before\",\"active_snoozed\",\"set_as_regular\"]\n",
    "numerical_cols =[col for col in feature_cols if col not in categorical_cols + binary_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_sizes = df.groupby('order_id')['outcome'].sum()\n",
    "filtered_orders=order_sizes[order_sizes>5].index\n",
    "df_filtered=df[df[\"order_id\"].isin(filtered_orders)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'order_date' and count unique 'order_id's\n",
    "daily_orders = df_filtered.groupby('order_date')['order_id'].nunique().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "daily_orders = daily_orders.rename(columns={'order_id': 'unique_order_count'})\n",
    "daily_orders['order_date'] = pd.to_datetime(daily_orders['order_date']).dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_orders['cumsum_ratio'] = daily_orders['unique_order_count'].cumsum() / daily_orders['unique_order_count'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_orders['cumsum_ratio'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_cutoff = daily_orders[daily_orders['cumsum_ratio'] <= 0.7].max()\n",
    "val_test_cutoff= daily_orders[daily_orders['cumsum_ratio'] <= 0.9].max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training from',daily_orders['order_date'].min())\n",
    "print(\"Train cutoff date:\",train_val_cutoff['order_date'])\n",
    "print(\"Validation cutoff date:\",val_test_cutoff['order_date'])\n",
    "print('Test until:',daily_orders['order_date'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.loc[:, 'order_date'] = pd.to_datetime(df_filtered['order_date']).dt.date\n",
    "\n",
    "train_val_cutoff = train_val_cutoff.date() if hasattr(train_val_cutoff, 'date') else train_val_cutoff\n",
    "val_test_cutoff = val_test_cutoff.date() if hasattr(val_test_cutoff, 'date') else val_test_cutoff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_cutoff = pd.to_datetime(train_val_cutoff.iloc[0]).date()\n",
    "val_test_cutoff = pd.to_datetime(val_test_cutoff.iloc[0]).date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_filtered[df_filtered['order_date'] <= train_val_cutoff]\n",
    "val_df = df_filtered[(df_filtered['order_date'] > train_val_cutoff) & \n",
    "                     (df_filtered['order_date'] <= val_test_cutoff)]\n",
    "\n",
    "test_df = df_filtered[df_filtered['order_date'] > val_test_cutoff]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first Baseline I am going to try to predict if something will be purchased (Outcome) based on the global popularity feature. \n",
    "\n",
    "In order to compare, we are going to start by preparing a function to plot Precision Recall and ROC curves, first for our baseline and then adding figures with our different models. \n",
    "\n",
    "The following function is for plting the curves. (Copy of Guille code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = train_df[['global_popularity']]\n",
    "y = train_df['outcome']\n",
    "\n",
    "\n",
    "y_pred_baseline = (X['global_popularity'])\n",
    "\n",
    "# Compute ROC curve and AUC for the baseline\n",
    "fpr, tpr, _ = roc_curve(y, y_pred_baseline)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Compute Precision-Recall curve and AUC for the baseline\n",
    "precision, recall, _ = precision_recall_curve(y, y_pred_baseline)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# ROC Curve\n",
    "ax[0].plot(fpr, tpr, color=\"blue\", label=f\"AUC = {roc_auc:.2f}\")\n",
    "ax[0].plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "ax[0].set_xlabel(\"False Positive Rate\")\n",
    "ax[0].set_ylabel(\"True Positive Rate\")\n",
    "ax[0].set_title(\"Baseline ROC Curve (Threshold on Global Popularity)\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax[1].plot(recall, precision, color=\"green\", label=f\"AUC = {pr_auc:.2f}\")\n",
    "ax[1].set_xlabel(\"Recall\")\n",
    "ax[1].set_ylabel(\"Precision\")\n",
    "ax[1].set_title(\"Baseline Precision-Recall Curve (Threshold on Global Popularity)\")\n",
    "ax[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "## Ridge\n",
    "\n",
    "\n",
    "We are going to start training our model. Its a good idea to start using our binary and numerical features. I have decided first of all to review if I can predict the outcome based on global popularity. For That, I am using Logistic Regression first of all with Ridge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_df[train_cols])\n",
    "X_val_scaled = scaler.transform(val_df[train_cols])\n",
    "\n",
    "# Define different values of C for comparison\n",
    "C_values = [1e-10, 1e-6, 1e-1, 1, 1000, 1000000]\n",
    "\n",
    "# Plot ROC and Precision-Recall curves for different values of C\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "for C in C_values:\n",
    "    # Fit the logistic regression model with L2 regularization (Ridge) and balanced class weights\n",
    "    ridge_model = LogisticRegression(penalty=\"l2\", C=C, class_weight=\"balanced\")\n",
    "    ridge_model.fit(X_train_scaled, train_df['outcome'])\n",
    "    \n",
    "    # Predict probabilities for validation set\n",
    "    val_proba = ridge_model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    # Compute ROC curve and AUC for validation set\n",
    "    fpr_val, tpr_val, _ = roc_curve(val_df['outcome'], val_proba)\n",
    "    roc_auc_val = auc(fpr_val, tpr_val)\n",
    "    \n",
    "    # Compute Precision-Recall curve and AUC for validation set\n",
    "    precision_val, recall_val, _ = precision_recall_curve(val_df['outcome'], val_proba)\n",
    "    pr_auc_val = auc(recall_val, precision_val)\n",
    "    \n",
    "    # Plot ROC Curve\n",
    "    ax[0].plot(fpr_val, tpr_val, label=f\"C = {C}, AUC = {roc_auc_val:.2f}\")\n",
    "    \n",
    "    # Plot Precision-Recall Curve\n",
    "    ax[1].plot(recall_val, precision_val, label=f\"C = {C}, AUC = {pr_auc_val:.2f}\")\n",
    "\n",
    "# Customize ROC Curve plot\n",
    "ax[0].plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "ax[0].set_xlabel(\"False Positive Rate\")\n",
    "ax[0].set_ylabel(\"True Positive Rate\")\n",
    "ax[0].set_title(\"Validation ROC Curve - Logistic Regression (Ridge)\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Customize Precision-Recall Curve plot\n",
    "ax[1].set_xlabel(\"Recall\")\n",
    "ax[1].set_ylabel(\"Precision\")\n",
    "ax[1].set_title(\"Validation Precision-Recall Curve - Logistic Regression (Ridge)\")\n",
    "ax[1].legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['outcome'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, this is the same than our baseline. So that should be that the ridge regression is not doing much here. We can appreciate that adjusting the level of regularisation in our model is not changing the AUC in the ROC curve, and also not changing much the precision-recall curve. \n",
    "\n",
    "Something looks off. I would expect some kind of change when changing the level of Regularisation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different values of C for comparison\n",
    "C_values = [1e-10, 1e-6, 1e-1, 1, 1000, 1000000]\n",
    "\n",
    "# Plot ROC and Precision-Recall curves for different values of C\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "for C in C_values:\n",
    "    # Fit the logistic regression model with L1 regularization (Lasso)\n",
    "    lasso_model = LogisticRegression(penalty=\"l1\", C=C, solver=\"saga\")\n",
    "    lasso_model.fit(X_scaled, y)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_proba_lasso = lasso_model.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred_proba_lasso)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Compute Precision-Recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred_proba_lasso)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Plot ROC and Precision-Recall Curves\n",
    "    ax[0].plot(fpr, tpr, label=f\"C = {C}, AUC = {roc_auc:.2f}\")\n",
    "    ax[1].plot(recall, precision, label=f\"C = {C}, AUC = {pr_auc:.2f}\")\n",
    "\n",
    "# Customize and show plots\n",
    "ax[0].plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "ax[0].set_xlabel(\"False Positive Rate\")\n",
    "ax[0].set_ylabel(\"True Positive Rate\")\n",
    "ax[0].set_title(\"ROC Curve - Logistic Regression (Lasso)\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_xlabel(\"Recall\")\n",
    "ax[1].set_ylabel(\"Precision\")\n",
    "ax[1].set_title(\"Precision-Recall Curve - Logistic Regression (Lasso)\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am seing differences when changing the level of regularisation. What I do see is that very strong regularisation  (very small C), is giving me 0.5 AUC in the ROC curve, meaning that is not better than Random guessing. For that reason, I would select bigger values of C. Once again, a part from that, I am not seing any difference at all between my baseline and my Ridge or Lasso. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusions\n",
    "\n",
    "Only taking in my model Global Popularity to predict Outcome, and using Logistic Regression with Ridge and Lasso, is not he best way of building my model, since I am not getting any better prediction other than just using my baseline. \n",
    "\n",
    "For that reason, I would have to review few things: \n",
    "    - Am I using the correct features for my prediction?\n",
    "    - Am I using using the correct models for my predictions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zrive-ds-diBAY53P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
